{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/brain_images_dataset.zip'\n",
        "\n",
        "extract_path = '/content/dataset/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "def resize_image(image_path):\n",
        "    original_image = Image.open(image_path)\n",
        "    resized_image = original_image.resize((240, 240))\n",
        "    return resized_image\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset',\n",
        "    target_size=(240, 240),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset',\n",
        "    target_size=(240, 240),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(240, 240, 3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((4, 4)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "history = model.fit(train_generator, epochs=20, validation_data=validation_generator, callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset',\n",
        "    target_size=(240, 240),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4HD8vMV4eia",
        "outputId": "f7921874-1576-4960-ac0e-9b6d157b4112"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 406 images belonging to 3 classes.\n",
            "Found 100 images belonging to 3 classes.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 238, 238, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 238, 238, 32)      128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 59, 59, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 57, 57, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 57, 57, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 28, 28, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 26, 26, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 26, 26, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 13, 13, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 21632)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               5538048   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5632449 (21.49 MB)\n",
            "Trainable params: 5632001 (21.48 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "13/13 [==============================] - 47s 3s/step - loss: 5.6705 - accuracy: 0.2709 - val_loss: 1.0121 - val_accuracy: 0.4800 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 45s 3s/step - loss: 4.3182 - accuracy: 0.2488 - val_loss: 3.1436 - val_accuracy: 0.4900 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 46s 3s/step - loss: 3.2443 - accuracy: 0.2488 - val_loss: 2.7839 - val_accuracy: 0.5000 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 47s 3s/step - loss: 3.1753 - accuracy: 0.2340 - val_loss: 1.1551 - val_accuracy: 0.4700 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 44s 3s/step - loss: 3.0480 - accuracy: 0.2488 - val_loss: 2.5406 - val_accuracy: 0.3700 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 44s 3s/step - loss: 2.2336 - accuracy: 0.2882 - val_loss: 1.2114 - val_accuracy: 0.2200 - lr: 9.0484e-04\n",
            "Found 506 images belonging to 3 classes.\n",
            "16/16 [==============================] - 21s 1s/step - loss: 1.3053 - accuracy: 0.4565\n",
            "Test Accuracy: 45.65%\n",
            "Test Loss: 1.3053\n"
          ]
        }
      ]
    }
  ]
}